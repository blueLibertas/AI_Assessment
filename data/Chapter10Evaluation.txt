Chapter 10
Evaluation in Instructional Design: A Comparison of the Major Evaluation Models

One of the fundamental components of instructional design is evaluation. The purpose of this chapter is to describe several of the most influential and useful evaluation models.
In 1967 Michael Scriven coined the terms formative and summative evaluation. These concepts are still used today. Here are Scriven’s (1991) definitions of formative and summative evaluation:
Formative evaluation is evaluation designed, done, and intended to support the process of improvement, and normally commissioned or done by, and delivered to, someone who can make improvements. Summative evaluation is the rest of evaluation: In terms of intentions, it is evaluation done for, or by, any observers or decision makers (by contrast with developers) who need evaluative conclusions for any reasons besides development. (p. 20)
The evaluation processes described in early instructional design models incorporated two key features. First, testing should focus on the objectives that have been stated for the instruction. This is referred to as criterion-referenced (or objective-referenced) testing. The argument is made that the assessment instruments for systematically designed instruction should focus on the skills that the learners have been told will be taught in the instruction. The purpose of testing is not to sort the learners to assign grades, but rather to determine the extent to which each objective in the instruction has been mastered. Assessments, be they multiple-choice items, essays, or products developed by the learners, should require learners to demonstrate the skills as they are described in the objectives in the instruction.
The second feature is a focus on the learners as the primary source of data for making decisions about the instruction. Although subject-matter experts (SMEs) are typically members of the instructional design team, they cannot always accurately predict which instructional strategies will be effective. Formative evaluation in instructional design should include a SME review, and that of an editor, but the major source of input to this process is the learner. Formative evaluation focuses on learners’ ability to learn from the instruction, and to enjoy it.

Defining Evaluation

To begin, we provide a formal definition of evaluation. Because of the prominence of Scriven’s work in evaluation, we will use his popular definition (Scriven, 1991, p. 139): “Evaluation is the process of determining the merit, worth, and value of things, and evaluations are the products of that process.” By merit Scriven is referring to the “intrinsic value” of the evaluation object or evaluand. By worth, Scriven is referring to the “market value” of the evaluand or its value to a stakeholder, an organization, or some other collective. By value, Scriven has in mind the idea that evaluation always involves the making of evaluative judgments. Scriven contends that this valuing process operates for both formative and summative evaluation. 
Scriven (1980) also provides a “logic of evaluation” that includes four steps. First, select the criteria of merit or worth. Second, set specific performance standards (i.e., the level of performance required) for your criteria. Third, collect performance data and compare the level of observed performance with the level of required performance dictated by the performance standards. Fourth, make the evaluative (i.e., value) judgments. In short, evaluation is about identifying criteria of merit and worth, setting standards, collecting data, and making value judgments.

The Major Evaluation Models

Many evaluation models were developed in the 1970s and 1980s. These evaluation models were to have a profound impact on how designers would come to use the evaluation process. The models were used on projects that included extensive development work, multiple organizations and agencies, and multiple forms of instructional delivery. These projects tended to have large budgets and many staff members, and were often housed in universities. The projects had multiple goals that were to be achieved over time. Examples were teacher corps projects aimed at reforming teacher education and math projects that attempted to redefine what and how children learned about mathematics. These projects often employed “new” models of evaluation. Perhaps the most influential model of that era was the CIPP model developed by Stufflebeam (1971).

Stufflebeam’s CIPP Evaluation Model
The CIPP acronym stands for Context, Input, Process, and Product. These are four distinct types of evaluation, and they all can be done in a single comprehensive evaluation or a single type can be done as a stand-alone evaluation.
Context evaluation is the assessment of the environment in which an innovation or program will be used, to determine the need and objectives for the innovation and to identify the factors in the environment that will impact the success of its use. This analysis is frequently called a needs assessment, and it is used in making program planning decisions. According to Stufflebeam’s CIPP model, the evaluator should be present from the beginning of the project, and should assist in the conduct of the needs assessment.
The second step or component of the CIPP model is input evaluation. Here, evaluation questions are raised about the resources that will be used to develop and conduct the innovation/program. What people, funds, space, and equipment will be available for the project? Will these be sufficient to produce the desired results? Is the conceptualization of the program adequate? Will the program design produce the desired outcomes? Are the program benefits expected to outweigh the costs of the prospective innovation/program? This type of evaluation is helpful in making program structuring decisions. The evaluator should play a key role in input evaluation.
The third step or component of CIPP is process evaluation. This corresponds closely to formative evaluation. Process evaluation is used to examine the ways in which an innovation/program is being developed, the way it is implemented, the initial effectiveness, and the effectiveness after revisions. Data are collected to inform the project leader (and other program personnel) about the status of the project, how it is implemented, whether it meets legal and conceptual guidelines, and how the innovation is revised to meet the implementation objectives. Process evaluation is used to make implementation decisions.
The fourth component of CIPP is product evaluation, which focuses on the success of the innovation/program in producing the desired outcomes. Product evaluation includes measuring the outcome variables specified in the program objectives, identifying unintended outcomes, assessing program merit, and conducting cost analyses. Product evaluation is useful in making summative evaluation decisions (e.g., “What is the overall merit and worth of the program? Should it be continued?”).
Introduction of the CIPP model to instructional design changed the involvement of the evaluator in the development process. The evaluator became a member of the project team. Furthermore, evaluation was no longer something that just happens at the end of a project, but became a formal process continuing throughout the life of a project.

Rossi’s Five-Domain Evaluation Model
Starting in the late 1970s and continuing to today, Peter Rossi and his colleagues developed a useful evaluation model (Rossi, Lipsey, & Freeman, 2004). According to this model, each evaluation should be tailored to fit local needs, resources, and type of program. This includes tailoring the evaluation questions (“What is the evaluation purpose?” “What specifically needs to be evaluated?”), methods and procedures (selecting those that balance feasibility and rigor), and the nature of the evaluator–stakeholder relationship (“Who should be involved?” “What level of participation is desired?” “Should an internal or an external/independent evaluator be used?”). For Rossi, the evaluation questions constitute the core, from which the rest of the evaluation evolves. Therefore, it is essential that you and the key stakeholders construct a clear and agreed upon set of evaluation questions.
The Rossi model emphasizes five primary evaluation domains. Any or all domains can be conducted in an evaluation. First is needs assessment, which addresses this question, “Is there a need for this type of program in this context?” A need is the gap between the actual and desired state of affairs. Second is program theory assessment, which addresses this question, “Is the program conceptualized in a way that it should work?” It is the evaluator’s job to help the client explicate the theory (how and why the program operates and produces the desired outcomes) if it is not currently documented. If a program is not based on sound social, psychological, and educational theory, it cannot be expected to work—this problem is called theory failure. Third is implementation assessment, which addresses this question: “Was this program implemented properly and according to the program manual?” If a program is not properly operated and delivered, it can’t succeed—this problem is called implementation failure.
The fourth evaluation domain is synonymous with the traditional social science approach to evaluation, and the fifth domain is synonymous with the economic approach to evaluation. The fourth domain, impact assessment, addresses this question: “Did this program have an impact on its intended targets?” This is the question of cause and effect. To establish cause and effect, you should use a strong experimental research design (if possible). The fifth domain, efficiency assessment, addresses this question: “Is the program cost effective?” It is possible that a particular program has an impact but it is not cost effective. For example, the return on investment might be negative, the costs might outweigh the benefits, or the program might not be as efficient as a competitive program. The efficiency ratios used in these types of analysis are explained in a footnote.2 Because of the importance of program theory we next discuss Huey Chen’s theory-driven evaluation.

Chen’s Theory-Driven Evaluation Model
Theory-driven evaluation (TDE) is an evaluation approach where evaluators and stakeholders determine how and why an intervention/program works as an important part of the evaluation (Chen, 2015). The TD evaluator helps articulate, evaluate, and improve the program theory, which is the stakeholders’ implicit and explicit assumptions about how a program responds to a problem and the actions required to solve the problem (Chen, 2015). An advantage of theory-driven evaluation is that it uncovers and articulates the areas that “black-box evaluation” (which focuses only on desired outcomes and not the processes between input and output) and “method-driven evaluation” (which emphasizes prioritized research methods and focuses only on impact) do not.
The paradigmatic TDE program theory schema is shown in Figure 10.1. A program theory includes an action model and a change model. The program theory is used to drive the theory-driven approach for program planning and evaluation (Chen, 2015). An action model is the program’s systematic plan for organizing resources, settings, staff, and support organizations to deliver intervention services and to reach the target population. An action model includes the following: (1) an intervention protocol and service delivery protocol (which details an intervention’s operating procedures and perspective and includes the steps needed to deliver the intervention in the field), (2) implementing organization(s) (i.e., the organization(s) that allocates resources, recruits, supervises, and trains implementers and coordinates activities), (3) implementers (i.e., the persons who deliver services to clients), (4) associated organizations and community partners (i.e., interested interorganizational stakeholders, to establish collaboration between community partners and associate organizations), (5) ecological context (i.e., the segment of the environment that interacts directly with the program), and (6) the target population (i.e., the group of people receiving the intended services) (Chen, 2015). The action model includes a lot of useful information, and it is an important tool to be constructed and used during the planning and conduct of a TDE.
The change model is the program’s description of the causal processes and outcomes required to resolve the identified problem(s). A change model includes three major elements: (1) the intervention (i.e., activity aimed directly at changing a problem), (2) causal determinants (i.e., causes of the problem or leverages, moderator variables, and intervening variables), and (3) program outcomes (i.e., changes on targeted proximal and distal dependent variables) (Chen, 2015). As shown in Figure 10.1, appropriate implementation of the components of the action model is critical for the change model’s transformation process to be activated (Chen, 2015).
We now provide a brief example of a TDE, illustrating the program theory for a technology integration program at a university in the southeastern United States.3 Specifically, the college of education at this university added VoiceThread to its campus learning-management-system toolkit. (VoiceThread is a software program that allows images, documents, and videos to be added to asynchronous discussion forums in online courses.) Figure 10.2 shows the program theory, including the action and change models. Take a moment now to examine the content in that figure.
The evaluation involved the following processes. The evaluator served as facilitator of the TDE process, helping outline the program theory and assumptions of the faculty and administration in the college of education and the computer center. The evaluator used the working-group-meeting process (Chen, 2015) to articulate and develop the program theory by interacting with representatives from the college of education, the instructional design project team, computer services, university faculty, the teaching and learning center, and university administrators. In this evaluation, forward reasoning was used to construct the action model (i.e., general program goals were articulated to determine the action model); backward reasoning (i.e., movement from the change model to the action model to determine program goals) was used to construct the change model; and the two approaches were integrated to determine the overall program theory. Initial program implementation was evaluated using constructive evaluation, which included a troubleshooting strategy (specifically, formative evaluation and program review/development meeting) and a development partnership strategy (where program stakeholders and the program evaluator collaborated). This was intended for internal use to immediately address the problems in the program. Next, using a hybrid process evaluation, the evaluator collected data on how the intervention was implemented. Finally, outcome data were examined to determine whether implementation contributed to changes on the outcome measures.
The evaluation included communicating with stakeholders on the procedures of the evaluation, the program plan and the action model, as well as collecting data using a combination of qualitative and quantitative methods (Chen, 2015). In this evaluation, results indicated that the program theory was sound and implementation produced the desired results on several outcome measures. The results and action plan were first presented to the dean of the college of education (who commissioned the evaluation) and then to the other stakeholders.

Kirkpatrick’s Training Evaluation Model
Kirkpatrick’s model was published initially in four articles in 1959 and is still frequently used today.4 Kirkpatrick specifically developed his model for training evaluation. What he originally referred to as steps later became the four levels of evaluation. Evaluators might only conduct evaluations at the early steps or they might evaluate at all four levels. The early levels of evaluation are useful by themselves, and they are useful in helping one interpret evaluation results from the higher levels. For example, one reason transfer of training (level 3) might not take place is because learning of the skills (level 2) never took place; likewise, satisfaction (level 1) is often required if learning (level 2) and other results (levels 3 and 4) are to occur.

Level 1: Reaction. Kirkpatrick’s first level is the assessment of learners’ reactions or attitudes toward the learning experience. Anonymous questionnaires should be used to get honest reactions from learners about the training. These reactions, along with those of the training director, are used to evaluate the instruction, but should not serve as the only type of evaluation. It is generally assumed that if learners do not like the instruction, it is unlikely that they will learn from it. Although level 1 evaluation is used to study the reactions of participants in training programs, it is important to understand that data can be collected on more than just a single overall reaction or customer satisfaction with the program (e.g., “How satisfied were you with the training event?”) Detailed level1 information should also be collected about training components and experiences (such as the instructor, the topics, the presentation style, perceived engagement, the schedule, the facility, the learning activities, and how engaged participants felt during the training event). We recommend also including open-ended items (i.e., where respondents respond in their own words): (1) “What do you believe are the three most important strengths of the training program?” and (2) “What do you believe are the three most important weaknesses of the training program?” It is usually best to use a mixture of open-ended items (such as the two questions just provided) and closed-ended items (such as providing a statement or item stem such as, “The material covered in the program was relevant to my job,” and asking respondents to use a fourpoint rating scale, such as very dissatisfied, dissatisfied, satisfied, very satisfied).5 The research design typically used for level 1 evaluation is the one-group posttest-only design (Table 10.1).

Level 2: Learning. In level 2 evaluation, the goal is to determine what the participants in the training program learned as well as their confidence that they can use it later and their commitment to use it later. By learning, Kirkpatrick (2006) has in mind “the extent to which participants change attitudes, improve knowledge, and/or increase skill as a result of attending the program” (p. 20). Training events will be focused on creating a combination of knowledge (“I know it”), skills (“I can do it”), attitudes (“I believe this will be worthwhile to do on the job”), confidence (”I think I can use it on my job”), and commitment (“I intend to use it later on my job”).
In addition to the perceptions just listed, level 2 evaluation should always include measurement of what specifically was covered in the training event and listed on the learning objectives. The tests should cover the material that was presented to the learners in order to have a valid measure of the amount of learning that has taken place. Knowledge is typically measured with an achievement test (i.e., a test designed to measure the degree of knowledge learning that has taken place after a person has been exposed to a specific learning experience); skills are typically measured with a performance test (i.e., a testing situation where test takers demonstrate some real-life behavior such as creating a product or performing a process); and attitudes and perceptions are typically measured with a questionnaire (i.e., a self-report data-collection instrument filled out by research participants designed to measure, in this case, the attitudes targeted for change in the training event). The one-group pretest-posttest design is often sufficient for a level 2 evaluation. As you can see in Table 10.1, this research design involves a pretest and posttest measurement of the training group participants on the outcome(s) of interest. The estimate of learning improvement is the difference between the pretest and posttest scores. Kirkpatrick appropriately recommends that a control group also be used when possible in level 2 evaluation because it allows stronger inferences about causation. This typically means that you will use the nonequivalent comparison-group design shown in Table 10.1 to demonstrate that learning has occurred as a result of the instruction. Learning data are not only helpful for documenting learning, but also for training directors in justifying their training function in their organizations.

Level 3: Behavior (transfer of training). Here the evaluator’s goal is to determine whether the training program participants change their on-the-job behavior (OJB) as a result of having participated in the training program. Learning in the classroom or other training setting is no guarantee that a person will demonstrate those same skills in the real-world job setting. The training director should conduct a follow-up evaluation several months after the training to determine whether the knowledge, attitudes, and skills learned are being used on the job. Kirkpatrick (2006) identifies five environments that affect transfer of training: (1) preventing environments (e.g., where the trainee’s supervisor does not allow the trainee to use the new knowledge, attitudes, or skills), (2) discouraging environments (e.g., where the supervisor discourages use of the new knowledge, attitudes, or skills), (3) neutral environments (e.g., where the supervisor does not acknowledge that the training ever took place), (4) encouraging environments (e.g., where the supervisor encourages the trainee to use new knowledge, attitudes, and skills on the job), and (5) requiring environments (e.g., where the supervisor monitors and requires use of the new knowledge, attitudes, and skills in the work environment).
To determine whether the knowledge, skills, and attitudes are being used on the job, and how well, it is necessary to contact the learners and their supervisors, peers, and subordinates. Kirkpatrick oftentimes seems satisfied with the use of what we call a retrospective survey design (asking questions about the past in relation to the present) to measure transfer of training. A retrospective survey involves interviewing or having trainees and their supervisors, peers, and subordinates fill out questionnaires several weeks and months after the training event to measure their perceptions about whether the trainees are applying what they learned. To provide a more objective indication of transfer to the workplace, Kirkpatrick suggests using designs 2, 3, and 4 (shown in Table 10.1). Level 3 evaluation is usually more difficult to conduct than lower level evaluations, but the resulting information is important to decision makers. If no transfer takes place then one cannot expect to have level 4 outcomes, which is the original reason for conducting the training.

Level 4: Results. Here the evaluator’s goal is to find out if the training leads to “final results.” Level 4 outcomes include any leading indicators and outcomes that affect the performance of the organization. Some desired organizational, financial, and employee results include reduced costs, higher quality of work, increased production, lower rates of employee turnover, lower absenteeism, fewer wasted resources, improved quality of work life, improved human relations, improved organizational communication, increased sales, few grievances, higher worker morale, fewer accidents, increased job satisfaction, and, importantly, increased profits.
Kirkpatrick acknowledges the difficulty of validating the relationship between training and level 4 outcomes. Because so many extraneous factors other than the training can influence level 4 outcomes, stronger research designs are needed (see designs 3 and 4 in Table 10.1). Unfortunately, implementation of these designs can be difficult and expensive. Nonetheless, it was Kirkpatrick’s hope that training directors would attempt to conduct sound level 4 evaluations and thus enhance the status of training programs.

Brinkerhoff’s Success Case Method
The next evaluation model is more specialized than the previous models. It focuses on finding out what about a training or other organizational intervention worked. According to its founder, Robert Brinkerhoff, the success case method (SCM) “is a quick and simple process that combines analysis of extreme groups with case study and storytelling . . . to find out how well some organizational initiative (e.g., a training program, a new work method) is working” (Brinkerhoff, 2005, p. 401). The SCM uses the commonsense idea that an effective way to determine “what works” is to examine successful cases and compare them to other cases. The SCM recognizes the organizational embeddedness of programs and seeks to explicate personal and contextual factors that differentiate effective from ineffective program use and results. This method is popular in human performance improvement (HPI) because it works well with training and nontraining interventions (Surry & Stanfield, 2008). SCM follows five steps (Brinkerhoff, 2003). First, you (the evaluator) focus and plan the success case study. You must identify and work with stakeholders to define the program to be evaluated, explicate its purpose, and discuss the nature of the SC approach to evaluation. You must work with stakeholders to determine their interests and concerns and obtain agreement on the budget and time frame for the study. Finally, this is when the study design is constructed and agreed upon.

Second, construct a visual impact model. This includes explicating the major program goals and listing all impacts/outcomes that are hoped for or are expected to result from the program. The far left side of a typical depiction of an impact model lists “capabilities” (e.g., knowledge and skills that should be provided by the program); these are similar to Kirkpatrick’s level 2 learning outcomes. The far right depicts “business goals” that are expected to result from the program; these are similar to Kirkpatrick’s level 4 results outcomes. The middle columns of a typical impact model include behaviors and organizational and environmental conditions that must be present to achieve the desired business goals. These might include critical actions (i.e., applications of the capabilities) and key intermediate results (e.g., supervisory, environmental, and client outcomes). An impact model is helpful for knowing what to include in your questionnaire to be used in the next step.

Third, conduct a survey research study to identify the best (i.e., success) cases and the worst cases. Unlike most survey research, responses are not anonymous because the purpose is to identify individuals. Data are collected from everyone if there are fewer than 100 people in the population; otherwise, a random sample is drawn.6 The survey instrument (i.e., the questionnaire) is usually quite short, unless you and the client decide to collect additional evaluation information.7 Three key questions for the questionnaire are: (1) “To what extent have you been able to use the [insert name of program here] to achieve success on [insert overall business goal here]?” then (2) “Who is having a lot of success in using the [insert program name]?” and (3) “Who is having the least success in using the [insert program name]?” The survey data can be supplemented with performance records and any other information that might help you to locate success cases (e.g., word of mouth, customer satisfaction reports).

Fourth, schedule and conduct in-depth interviews (usually via the telephone for approximately 45 minutes per interview) with multiple success cases. Sometimes you will also want to interview a few nonsuccess cases. The purpose of the fourth step is to gain detailed information necessary for documenting, with empirical evidence, the success case attributes and stories. During the interviews you should discuss categories of successful use and identify facilitating and inhibiting use factors. During the success case interviews, Brinkerhoff (2003) recommends that you address the following information categories:

a. What was used that worked (i.e., what information/strategies/skills, when, how, with whom, and where)?
b. What successful results/outcomes were achieved, and how did they make a difference?
c. What good did it do (i.e., value)?
d. What factors helped produce the successful results?
e. What additional suggestions does the interviewee have for improvement?

During nonsuccess case interviews, the focus is on barriers and reasons for lack of use of what was expected to be provided by the program. You should also obtain suggestions for increasing future use. During and after all interviews, it is important to obtain evidence and carefully document the validity of the findings.

Fifth, document and communicate the evaluation findings. In Brinkerhoff’s words, this is where you “tell the story.” The report will include detailed data and evidence as well as rich narrative communicating how the program was successful and how it can be made even more successful in the future. Again, provide sufficient evidence so that the story is credible. Brinkerhoff (2003, pp. 169–172) recommends that you address the following six conclusions in the final report:

1. What worthwhile actions and results, if any, is the program helping to produce?
2. Are some parts of the program working better than others?
3. What environment factors are helping support success, and what factors are getting in the way?
4. How widespread is the scope of success?
5. What is the ROI (return on investment) of the new program?
6. How much more additional value could be derived from the program?

Brinkerhoff emphasizes that success case evaluation results must be used if long-term and company-wide success is to result. The most important strategy for ensuring employee “buy-in” and use of evaluation results and recommendations is to incorporate employee participation into all stages of the evaluation. For a model showing many of the factors that affect evaluation use, read Johnson (1998). Because of the importance of evaluation, the next and final evaluation model is constructed around the concept of evaluation use.

Patton’s Utilization-Focused Evaluation
Evaluation process and findings are of no value unless they are used. If an evaluation is not likely to be used in any way, one should not conduct the evaluation. In the 1970s, Michael Patton introduced the utilization-focused evaluation (U-FE) model, and today it is in the forth book edition (which is much updated/expanded from earlier editions) (Patton, 2008). The U-FE model is “evaluation done for and with specific intended users for specific, intended uses” (Patton, 2008, p. 37). The cardinal rule in U-FE is that the utility of an evaluation is to be judged by the degree to which it is used. The evaluator focuses on use from the beginning until the end of the evaluation, and during that time he or she continually facilitates use, and organizational learning or any other process that helps ensure that the evaluation results will continue to be used once the evaluator leaves the organization. Process use occurs when clients learn the “logic” of evaluation and appreciate its use in the organization. Process use empowers organizational members to become internal evaluators.
The U-FE model follows several steps. Because U-FE is a participatory evaluation approach, the client and primary users will be actively involved in the structuring, conducting, interpreting, and using evaluation and its results. Here are the major steps:

1. Conduct a readiness assessment (i.e., determine if the organization and its leaders are ready and able to commit to U-FE).
2. Identify the “primary intended users” and develop a working relationship with them (i.e., primary intended users are the key individuals in the organization that have a stake in the evaluation and have the ability, credibility, power, and teachability to work with a U-FE evaluator in conducting an evaluation and using the results).
3. Conduct a situational analysis (i.e., examine the political context, stakeholder interests, and potential barriers and supports to use).
4. Identify the “primary intended uses” (e.g., program improvement, making major decisions, generating knowledge, and process use or empowering stakeholders to know how to conduct evaluations once the evaluator has left).
5. Focus the evaluation (i.e., identify stakeholders’ high priority issues and questions).
6. Design the evaluation (one that is feasible and will produce results that are credible, believable, valid, and actionable).
7. Collect, analyze, and interpret the evaluation data (and remember to use multiple methods and sources of evidence).
8. Continually facilitate evaluation use (e.g., interim findings might be disseminated to the organization, rather than waiting for the “final written report”; U-FE does not stop with the final report, because the evaluator must work with the organization until the findings are used).
9. Conduct a metaevaluation (i.e., an evaluation of the evaluation, to determine (a) the degree to which intended use was achieved, (b) whether additional uses occurred, and (c) whether any misuses and/or unintended consequences occurred; the evaluation is successful only if the findings are used effectively).

Utilization-focused evaluation is a full approach to evaluation (Patton, 2008), but it also is an excellent approach to complement any of the other evaluation models presented in this chapter. Again, an evaluation that is not used is of little use to an organization; therefore, it is wise to consider the principles provided in the U-FE model.
To become an effective utilization-focused evaluator, we recommend that you take courses in human performance improvement, leadership and management, industrial-organizational psychology, organizational development, organizational communication, and organizational behavior. If you become a utilization-focused evaluator, it will be your job to continually facilitate use, starting from the moment you enter the organization. You will attempt to facilitate use by helping transform the state of the organization so that it is in better shape when you leave than when you entered.

Conclusion

Evaluation is important because it is a part of all major models of instructional design, it is a required skill for human performance technologists, it provides a systematic procedure for making value judgments about programs and products, and it can help improve employee and organizational performance. Some instructional designers will elect to specialize in evaluation and become full-time program evaluators. To learn more about evaluation as a profession, go to the website of the American Evaluation Association (http://www.eval.org/).
Stufflebeam’s CIPP model focuses on program context (for planning decisions), inputs (for program structuring decisions), process (for implementation decisions), and product (for summative decisions). Rossi’s evaluation model focuses on tailoring each evaluation to local needs and focusing on one or more of the following domains: needs, theory, process/implementation, impact, and efficiency. Chen’s TDE model focuses on articulating a program theory so that one will know how and why a program operates well or poorly. Kirkpatrick’s model focuses on four levels of outcomes, including reactions, learning, transfer of learning, and business results. Brinkerhoff’s success case model focuses on locating and understanding program successes so that success can become more widespread in the organization. Patton’s U-FE model focuses on conducting evaluations that will be used.

Summary of Key Principles
1. Evaluation is the process of determining the merit, worth, and value of things, and evaluations are the products of that process.
2. Formative evaluation focuses on improving the evaluation object, and summative evaluation focuses on determining the overall effectiveness, usefulness, or worth of the evaluation object.
3. Rossi shows that evaluation, broadly conceived, can include needs assessment, theory assessment, implementation assessment, impact assessment, and efficiency assessment.
4. Chen shows how an evaluator working with program staff articulates the program theory (includes an action model and change model) to facilitate development and evaluation of a program.
5. Kirkpatrick shows that training evaluations should examine participants’ reactions, their learning (of knowledge, skills, and attitudes), their use of learning when they return to the workplace, and business results.
6. Brinkerhoff shows that organizational profits can be increased by learning from success cases and applying knowledge gained from studying these cases.
7. It is important that evaluation findings are used, rather than “filed away,” and Patton has developed an evaluation model specifically focused on producing evaluation use.
8. One effective way to increase the use of evaluation findings is through employee/stakeholder participation in the evaluation process.
