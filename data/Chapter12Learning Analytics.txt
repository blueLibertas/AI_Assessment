Chapter 12
An Introduction to Learning Analytics

It is undeniable that we are living in a data-driven world. Amazon, iTunes, and Netflix seem to know what our favorite books, electronic gadgets, music, and movies are before we ourselves know. Google can predict what we are searching for before we even finish typing. And our grocery stores send us coupons for the exact products we purchase every week. In short, the amount of data we have at our fingertips is astonishing. So why not use that data to make instructional design decisions? Data-informed decision making involves making use of data to inform judgments (Jones, 2012; Long & Siemens, 2011; Picciano, 2012). As an instructional designer, you make decisions about course design and delivery based on evidence. For example, you likely know that active-learning activities can lead to deeper processing of content, which can lead to deeper learning (e.g., Yoder & Hochevar, 2005). Evidence such as this leads to the creation and inclusion of active-learning activities in a course. Likewise, it would be helpful to make course-design decisions based on evidence from prior performance in the course or on student characteristics. Perhaps students who have certain skills or abilities or prior coursework perform better in the class or set of classes than those who do not. The use of data to make decisions about course design is one of the goals of learning analytics. In this chapter, we provide a brief overview of learning analytics, including various tools to track, extract, and analyze data. We will also explore its uses and applications, goals, and examples. Furthermore, we discuss why instructional designers will want to make use of learning analytics. Any discussion of learning analytics is not complete without a thorough discussion of the issues and concerns with the use of this type of data, so we will discuss that as well.

An Overview of Learning Analytics 

Although the term analytics is relatively new in the field of education, the concept is not. Analytics involves collecting and exploring data sets to search for meaningful patterns. For an educator, an example of this could be reviewing student attendance and grade data and observing that students with poor attendance often have poor grades. The discovery of these patterns may give insight into observed behavior and prompt actions to address problems or otherwise improve a situation. The growing interest in analytics is driven by the ubiquitous nature of modern technology and deluge of data it generates. An ever expanding range of tools and techniques are being developed to collect, organize, analyze, and visualize this data. With knowledge of these tools, instructional designers will be able to make use of learning analytics to improve learning experiences. In education, learning analytics can be defined as “the measurement, collection, analysis, and reporting of data about learners and their contexts, for the purpose of understanding and optimizing learning and the environment in which it occurs” (Long & Siemens, 2011, p. 32). Learning analytics are being explored across all aspects of the educational environment. At the course and department levels, instructors and instructional designers look for insight on how students interact with course material provided in a learning management system (LMS), to explore ways to improve specific outcomes for individual activities within a class, for a class as a whole, or for a department or program. Long and Siemens (2011) use the term “academic analytics” to distinguish analytics that focus on institutions and, more broadly, higher education, including recruitment and retention of students (Campbell, DeBlois, & Oblinger, 2007) and selection of courses and programs of study by students (Denley, 2014). While this narrows the focus, it still includes a wide range of practices, such as inspecting student data in a gradebook, examining student access information in a LMS, assessing adaptive learning (Feldstein, 2013), and other activities. To gain a sense of the use of and value of learning analytics, consider Puget’s analytics landscape (Puget, 2015). It distinguishes four “maturity levels” (Table 12.1) and provides the instructional designer with a useful path for progressing from less complex uses of learning analytics to more mature and complex levels. When the course is first designed, the instructional designer can use best practices to focus on identifying the data that can be collected (descriptive analytics). After the course has been delivered (or even as the course is being delivered) the data can be explored to look for patterns that may explain observed outcomes (diagnostic analytics). These patterns can then be analyzed to explore and test predictive hypothesis that can be validated the next time the course is delivered (predictive analytics). If a hypothesis is found to have predictive power, then efforts can be made to identify students who need to be encouraged to take specific actions (prescriptive analytics).

Goals of Learning Analytics 

Learning analytics can have multiple goals and can be pitched at several levels, including course, department, and institutional levels. Instructional designers should keep these goals in mind during course planning and design. Liñán, Alejandro, and Pérez (2015) suggest that the main goal of learning analytics is to “extract information from educational data to support education- related decision making” (p. 99). While institutions have access to data for improved reporting, educators can use learning analytics to improve the student learning experience. Larusson and White (2014) summarize the goals of learning analytics: 
Whether it be through the use of statistical techniques and predictive modeling, interactive visualizations, or taxonomies and frameworks, the ultimate goal is to optimize both student and faculty performance, to refine pedagogical strategies, to streamline institutional costs, to determine students’ engagement with the course material, to highlight potentially struggling students (and to alter pedagogy accordingly) to fine-tune grading systems using real-time analysis, and to allow instructors to judge their own educational efficacy. (p. 1–2)
Of course, in practice, the goals of learning analytics should support student success. This can be accomplished through data-driven initiatives focused on monitoring and documenting student learning, identifying at-risk candidates for intervention, improving pedagogy and instruction, and streamlining institutional processes. By understanding the goals of learning analytics, the data to be collected, and how it can be utilized, instructional designers are better prepared to support student success through informed design.

Types of Data 

As previously stated, the use of learning analytics serves many purposes and fulfills numerous academic and institutional goals. But what are these data and how does an instructional designer assist faculty in finding and using them? There are a number of different data sources. One type of data source is institutional (not course specific) in nature. In educational settings, there is a massive amount of institutional-type data to be mined and analyzed. Examples of such data include admissions data (e.g., SAT scores, high school GPA), proficiency and readiness data (e.g., online readiness scores), demographic data (e.g., demographic profile of students who dropout), and data on characteristics of successful students in the major. Instructional designers as well as faculty can make use of these data in the course design process. For example, Morris, Wu, and Finnegan (2005) found that high school GPA and SAT scores successfully predicted dropout (60 percent) and student completion (76 percent) rates in online courses.
Another type of data source, and the one most commonly used in learning analytics, is LMS data. In general, these data are not only readily available to faculty and instructional designers, but are very amenable to basic analysis. For example, the first column in Table 12.2 shows the type of data that are available in the LMS used at our institution (Canvas). For the most part, these data can be easily culled from the LMS and used in various analyses. For example, Smith, Lange, and Huston (2012) used such LMS data as login frequency, site engagement, student pace in the course, and assignment grades to predict course outcomes for individual students.
Instructor-generated data can also be a valuable source of data for learning analytics. The first column of Table 12.3 shows examples of the types of data that may or may not be available in a LMS, but that faculty and instructional designers can track and collect. For example, to determine the effects of sending a “checking in” email to students in an online course, Dietz (2015) examined the number and types of email responses. She found that the number of replies were positively correlated with final grades.
When thinking about data sources, it is also important to be mindful of what purpose the data might help serve; in other words, it is important to think about the goals of each specific learning analytic activity. As noted in an earlier section of this chapter, from an instructional design or faculty perspective, the goals can be varied, but might include a desire to track student engagement, assess various forms of student learning (e.g., knowledge acquisition, critical thinking skills, inquiry skills), measure feelings of community, assess retention, and so forth. The second column of Tables 12.2 and 12.3 provide suggested purposes for each of the various data sources. For example, if an instructional designer or faculty member was interested in the factors predicting student engagement, he or she might perform a social network analysis of the discussion board posts (Shum & Ferguson, 2012).

How to Use Learning Analytics 

Instructional designers now have a host of data available to improve the design of their courses at every level of the instructional design process. Course design is a continuous process, and a constant flow of incoming data allows for a solid foundation and improvement of courses and programs. Learning analytics adds another tool to the designer’s tool box. Learning-analytic information can be used in the initial design of a course or program or for the continuous improvement of an existing course or program. Data can also be used to improve the achievement of learning outcomes. If there is evidence of limited instructor-to-student engagement, for example, then interventions can provide instructors with strategies for increasing engagement. If there is evidence that students are not engaging with the content, the design of the course can be altered to include scaffolded checkpoints to incorporate continuous course touchpoints with feedback.
When thinking of how to use learning analytics, we find it helpful to be mindful of the goals of learning analytics, which then determine the level of data that are necessary to achieve those goals. At the institutional level, an examination of data is useful for developing an appreciation of educational backgrounds, experiences, and life status of the audience for the course or program. Useful questions that can be answered with institutional and departmental data are, What is the educational background of the students entering this course? What are other factors in the lives of these students that might influence their ability to learn? Where does this course fall in the program progression for most students? What are the specific skill averages (reading, writing, math, technical level) of the students when they take this course? For example, students who have not yet taken a research methods course, or have limited writing skills, will often struggle with a research paper assignment. These students may require additional scaffolding with support to successfully reach a specific learning outcome.
A key to continuous program improvement is to examine and analyze data at the program level. For example, completion data is useful in discovering which courses students find to be the most challenging, which courses cause students to exit a program, and which courses serve as gateway course for the program. A simple adjustment to the design of a program might be to change the order in which students move through the required courses of study. Similarly, an examination of key assessments in a program can provide essential data to ensure that multiple sections of courses with different instructors are achieving the intended student learning outcomes. Programmatic data might demonstrate patterns based on key assessments which can be used to continuously improve the course and subsequent student performance.
Increasing amounts of data are also available at the course level through the LMS as well as from electronic versions of textbooks, if one is being used (Van Horne, Russell, & Schuh, 2015). When designing or redesigning courses, these data make it easier to determine necessary areas for improvement. Historical data from a course that is being redesigned can be compared to courses in which students are successful and are satisfied with their experiences.
Finally, it is wise to consider data at the national or higher education level. It is known, for example, that students who do well on the initial exam of most courses tend to do well on subsequent work (Brown, 2012). Similarly, a more personalized approach to learning often leads to greater achievement of student learning outcomes. For example, Bloom explained that students perform much better with one-on-one or one-on-two tutoring (Bloom, 1984). It is important to design courses that allow faculty to utilize real-time data to personalize the experience for each student or better yet, for the student to “choose their own adventure” through the course.

Case Studies and Tools 

To gain a sense of the scope and ability of learning analytics, it is helpful to be aware of some case studies and tools. In this section, we provide examples of how institutions have used dashboards, institutional data, and database tools to improve student success and retention. Many institutions have created and/or used dashboard tools (see Verbert, Duval, Klerkx, Govaerts, & Santos, 2013, for a review). The goal of a dashboard is to provide a visualization of student progress and performance in a course (e.g., Baepler & Murdoch, 2010). Dashboards can be configured for students, instructors, or both. For example, Purdue University created SIGNALS, which extracts data and provides a dashboard for both students and faculty to track student progress.
There are also interesting examples of using institutional data to predict student success and retention. For example, Sinclair Community College developed their Student Success Plan (SSP) for advising and retention. Collection and analysis of these data allows them to track student development and improve student success. In a similar vein, the University of Maryland Baltimore Country makes use of a learning analytics tool built into their institution's LMS, which allows them to track student progress. Similarly, there are many examples of institutions that have successfully implemented student-focused learning analytics systems. These institutions use data to create a model of successful student behavior to which current students can be compared. When students deviate from the model, an intervention is triggered, and advisors, instructors, or other student services can be notified. Norris and Baer (2013) documented several institutions including Purdue, Rio Salado College, and Arizona State University that have implemented predictive analytics to identify at-risk students for intervention. The University of Michigan uses an intervention program that, at the time it was designed, was based on fourteen years of historical student data. The system uses this data to identify students and recommends study resources and strategies (Wright, Mckay, Hershock, Miller, & Tritz, 2014). These institutions have reported success in identifying and intervening with at-risk students. 
Finally, there are a plethora of database tools that can be used to track student progress and predict student success. These include large databases such as dataTEL, DataShop, and Mucle (Verbert, Manouselis, Drachsler, & Duval, 2012) that have been or can be used for learning analytic projects. Social network tools include "Mzinga" or SNAPP (Social Networks Adapting Pedagogical Practice) can be used to quantify the level of participation in a network of learners. Similarly, "Gephi" provides a visualization of social network participation. 

Benefits of Using Learning Analytics 

A well-designed learning analytics program comes with a host of benefits that can be realized at both the institutional and classroom levels. For institutional administrators, the benefits include the ability to provide a clearer picture of the institution's health, leading to more efficient resource allocation and improved productivity (Long & Siemens, 2011). In the classroom, benefits include identifying at-risk students, measuring classroom success, gaining insight into student learning, and informing pedagogical choices. Students benefit from an analysis of their learning activities, leading to the recommendation of specific resources (Long & Siemens, 2011), and the development of customized study plans (Bichsel, 2012). These benefits are not only advantageous to students, but they also can support predictive models of student behavior, enabling educators and academic advisors to intervene, and instructional designers to improve course design. 
Learning analytics often uses a combination of longitudinal student data and LMS activity (Norris & Baer, 2013). In a case study on using learning analytics to engage in interventions, Smith et al. (2012) found that several kinds of student activities logged by the LMS correlated with course outcomes. By tracking log-in frequency, time spent in the course, and other activity data, instructors can intervene early in a course to help students improve their academic behaviors (Smith et al., 2012). These interventions not only help students stay enrolled, they also benefit the institution by helping it meet its education mission. 
Furthermore, the benefits of learning analytics extend to course design/redesign and pedagogy. Learning analytics can help educators identify and reinforce weak content areas as well as address student needs for additional instruction (Krumm, Waddington, Teasley, & Lonn, 2014). Not only can instructors monitor student progress, but both instructors and instructional designers can gain insight into the structure and content of their curriculum and make improvements in future course iterations (Liñán et al., 2015). Another benefit comes from informing students about their own learning processes (Scheffel, Drachsler, Stoyanov, & Specht, 2014). By sharing these data with students, they can reflect on their learning activities and behaviors and make adjustments if necessary.
As the study and use of learning analytics matures, new kinds of benefits will be realized. For example, Norris and Baer’s (2013) learning analytics framework not only involves the use of predictive analytics on campus, but extends data collection after graduation and into careers, leading to another potential area for applying student modeling. Additionally, academic advisors have become users of learning analytics systems, which have helped advisors keep track of advisees even when those students do not actively participate in the advising process (Krumm et al., 2014). The benefits of learning analytics impact the institution through improving the depth and richness of data reporting. Instructors and instructional designers benefit from gaining insight into student behavior as well as discovering areas for course improvements and pedagogical changes. Most importantly, students benefit with an improved learning experience based on a data-driven support structure.

Concerns/Issues

While learning analytics offers great promise for improving experiences and outcomes for students and instructors, there are several issues and concerns that need to be considered. Instructional designers should be aware of these concerns as they may be involved in addressing many of them. A fundamental set of concerns relates to culture and includes ethical, legal, and privacy matters. The globalization of education complicates these considerations by bringing together different national and cultural perspectives.
Ethically, analytics need to be seen as tools to support pedagogy rather than as an end in and of themselves (Greller & Drachsler, 2012). Learning analytics involve and impact human beings, so the ethical and legal constraints applied to research on human subjects should be considered (Belmont Report, 1979). Other cultural issues include concerns that learning analytics may be “Big Brother-ish” and lead to profiling of learners (Campbell et al., 2007), could reveal information before individuals wish to share the information (Duhigg, 2012), or lead individuals to feel that they are being monitored and manipulated for the benefit of others.
Questions about privacy are complicated by the fact that it is not often clear who has ownership of the data. The digital nature of data makes it easy to transfer data around the world, but legal protection for data changes at national borders. The nature of digital data also makes considerations of ownership difficult. If data is generated through a student’s participation, does the student have rights over the data? While data rights are poorly defined in the United States, the European Union has clear protection for an individual’s personal data (Council Directive 95/46/EC, 1995).
A related concern with learning analytics is that data used in analytics is often not collected specifically for use in the analytics process. For example, a LMS typically logs student actions like accessing a page or viewing a video. These data could be useful in learning analytics. However, typically all that is being recorded is that the student requested the page or video, not that the content was delivered. Similarly, apparent patterns in student participation in an online discussion may simply reflect the due dates for the assignment. This secondary use of data presents many challenges (Solove, 2008). A related issue involves the responsibility of acting on the data. If learning analytics can identify an intervention that will improve student success, is there an obligation for the institution to act? If so, who should act? Should the instructor or perhaps a dedicated learning support specialist respond? And who will be responsible for providing the resources to support necessary interventions?
The use of learning analytics broadens the skills and knowledge that instructional designers and instructors must understand. While detailed knowledge of data collection methods and statistical analysis may not be needed, instructional designers will need knowledge to understand that individual uses of learning analytics are appropriate for the context of a specific course. A special challenge here is that there can be wide variations in the number and demographics of students enrolled in a course. So, analytics that show good predictive power in one setting may have no predictive power in another setting. While this may seem daunting, many of the techniques used to introduce and support the use of technology in teaching could be applied to developing knowledge and promoting use of learning analytics.
A final concern is the current immaturity of learning analytics, which is to be expected for a new and developing field. One consequence of this is that before they can look for insights, instructional designers may need to be involved in data collection, management, and analysis. Instructional designers may spend more time finding or building tools, managing data, and resolving issues than they spend getting insights about how to improve a course. Adding to this challenge is that many of the tools and techniques developed for analytics outside the educational sector are intended for data sets several orders of magnitude larger than those from a class of twenty to one hundred students. A related aspect of this immaturity is that tools and data available often were not developed specifically for learning analytics. For example, data on student use of video content in a LMS may simply represent when the student requested the content, with no indication that the content was successfully displayed to the student, much less whether the student actually viewed the video. Similarly, a LMS may display the last time a student logged into the LMS, but from an analytics perspective, it would be more useful to display the elapsed time since the last login.
These issues and concerns will continue to evolve as learning analytics mature. Issues related to ethics, privacy, and use of data will be ongoing concerns at the institutional, national, and international levels. As the individuals most directly involved in designing and delivering learning experiences, instructional designers and instructors must be involved in discussing and addressing these concerns. Concerns about developing knowledge and tools to support learning analytics will also persist, but should diminish as research and exploration of learning analytics progresses.

Conclusion

There is no doubt that we increasingly live in a data-driven world of predictive data analytics. The ultimate goal is to use these data to improve the learning experiences of students through data-supported decision making and design. There are prescriptive analytics available at the course, program, institutional, and national levels that can be used in the initial design of a course as well as for continuous improvement of courses and programs. The potential benefits include better institutional and programmatic health and, of course, more successful students. Because learning analytics is still in the infancy stage, it might require a steep investment in planning, collecting, and analyzing data in order for the instructional designer to reap significant benefits. Care should be exercised in the collection, securing, and usage of all data to be sure that privacy and ethical issues are properly addressed. Learning analytics are here to stay and will soon be an expected part of the instructional designer’s arsenal.

Summary of Key Principles

1. The main goal of learning analytics is to use data-driven tools and procedures to improve student experiences. Student experiences can be improved through using data to improve administrative efficiency, inform pedagogy and curriculum, and advance student learning.
2. The sources and types of data amenable to learning analytics are many and varied. Although most data for learning analytics applications are culled from learning management systems, data can also be obtained from institutional records, e-texts or other learning applications, course evaluations, national data, and the instructor.
3. Learning analytics can be applied in design at the course, program, and institutional levels. Data can be used to drive the initial design and redesign of courses and programs while also influencing institutional directions and decisions.
4. To gain a sense of the utility and capability of learning analytics, consider modeling some of the tools or case studies described in this chapter. Many institutions have developed their own tools for learning-analytic purposes or have exploited the features of their learning management systems for student alerts or for predictive purposes. 
5. Many stakeholders benefit from the application of learning analytics, most importantly students. By developing tools that make effective use of learning analytics, behaviors can be modeled, at-risk students identified, and interventions implemented.
6. Before making use of learning analytics, instructional designers should be familiar with concerns about ethics, invasion of privacy, and profiling that could impact learning and the relationship between instructors and students.
